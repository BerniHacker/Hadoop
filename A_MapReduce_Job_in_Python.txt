Joining data with streaming using Python code and Cloudera VM.

1.

Opening a text editor, and copying the lines of the files (see the Files section at the bottom) into the text editor.

> gedit make_join2data.py
> gedit make_data_join2.txt

Creating one more directory on the HDFS file system:

> hdfs dfs -mkdir /user/cloudera/input3

Copying the text file from the local filesystem to the HDFS filesystem:

> hdfs dfs -put /home/cloudera/make_data_join2.txt /user/cloudera/input3

Entering the following to see that the indentations line up correctly in the .py files:

> more make_join2data.py

Entering the following to make the .py files executable:

> chmod +x make_join2data.py

Generating some datasets using the scripts as follows:

> sh make_data_join2.txt

EXECUTION

[cloudera@quickstart ~]$ gedit make_join2data.py
[cloudera@quickstart ~]$ gedit make_data_join2.txt
[cloudera@quickstart ~]$ hdfs dfs -mkdir /user/cloudera/input3
[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/make_data_join2.txt /user/cloudera/input3
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/input3
Found 1 items
-rw-r--r--   1 cloudera cloudera        333 2018-06-19 12:33 /user/cloudera/input3/make_data_join2.tx
[cloudera@quickstart ~]$ more make_join2data.py
...

PROVIDED FILES

                    *** make_join2data.py ***

#!/usr/bin/env python
import sys

# --------------------------------------------------------------------------
#  (make_join2data.py) Generate a random combination of titles and viewer counts, or channels
# this is a simple version of a congruential generator, 
#   not a great random generator but enough  
# --------------------------------------------------------------------------

chans   = ['ABC','DEF','CNO','NOX','YES','CAB','BAT','MAN','ZOO','XYZ','BOB']
sh1 =['Hot','Almost','Hourly','PostModern','Baked','Dumb','Cold','Surreal','Loud']
sh2 =['News','Show','Cooking','Sports','Games','Talking','Talking']
vwr =range(17,1053)

chvnm=sys.argv[1]  #get number argument, if its n, do numbers not channels,

lch=len(chans)
lsh1=len(sh1)
lsh2=len(sh2)
lvwr=len(vwr)
ci=1
s1=2
s2=3
vwi=4
ri=int(sys.argv[3])
for i in range(0,int(sys.argv[2])):  #arg 2 is the number of lines to output

    if chvnm=='n':  #no numuber
        print('{0}_{1},{2}'.format(sh1[s1],sh2[s2],chans[ci]))
    else:
        print('{0}_{1},{2}'.format(sh1[s1],sh2[s2],vwr[vwi])) 
    ci=(5*ci+ri) % lch   
    s1=(4*s1+ri) % lsh1
    s2=(3*s1+ri+i) % lsh2
    vwi=(2*vwi+ri+i) % lvwr
 
    if (vwi==4): vwi=5
	
                    *** make_data_join2.txt ***

python make_join2data.py y 1000 13 > join2_gennumA.txt
python make_join2data.py y 2000 17 > join2_gennumB.txt
python make_join2data.py y 3000 19 > join2_gennumC.txt
python make_join2data.py n 100  23 > join2_genchanA.txt
python make_join2data.py n 200  19 > join2_genchanB.txt
python make_join2data.py n 300  37 > join2_genchanC.txt	

2. Using HDFS commands to copy the 6 files created in step 1 into a new HDFS directory.

EXECUTION

[cloudera@quickstart ~]$ hdfs dfs -mkdir /user/cloudera/input4
[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/join2_gennumA.txt /user/cloudera/input4
[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/join2_gennumB.txt /user/cloudera/input4
[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/join2_gennumC.txt /user/cloudera/input4
[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/join2_genchanA.txt /user/cloudera/input4
[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/join2_genchanB.txt /user/cloudera/input4
[cloudera@quickstart ~]$ hdfs dfs -put /home/cloudera/join2_genchanC.txt /user/cloudera/input4
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/input4
Found 6 items
-rw-r--r--   1 cloudera cloudera       1714 2018-06-19 12:39 /user/cloudera/input4/join2_genchanA.txt
-rw-r--r--   1 cloudera cloudera       3430 2018-06-19 12:39 /user/cloudera/input4/join2_genchanB.txt
-rw-r--r--   1 cloudera cloudera       5152 2018-06-19 12:39 /user/cloudera/input4/join2_genchanC.txt
-rw-r--r--   1 cloudera cloudera      17114 2018-06-19 12:37 /user/cloudera/input4/join2_gennumA.txt
-rw-r--r--   1 cloudera cloudera      34245 2018-06-19 12:38 /user/cloudera/input4/join2_gennumB.txt
-rw-r--r--   1 cloudera cloudera      51400 2018-06-19 12:38 /user/cloudera/input4/join2_gennumC.txt
[cloudera@quickstart ~]$ 

3. The datasets generated in step 1 contain the following information:

join2_gennum*.txt consist of <TV show, count> (A TV show title and the number of viewers)

Example join2_gennum*.txt:

Almost_News, 25
Hourly_Show,30
Hot_Cooking,7
Almost_News, 35
Postmodern_Family,8
Baked_News,15
Dumb_Games,60
…

join2_genchan*.txt consists of <TV show title, channel> (A TV show title and the channel it was on)

Example join2_genchan*.txt:

Almost_News, ABC
Hourly_Show, COM
Hot_Cooking, FNT
Postmodern_Family, NBC
Baked_News, FNT
Dumb_Games, ABC
…

4. Implementing the following join request in Map/Reduce:

                    What is the total number of viewers for shows on ABC?

Note the following:
					
* The show-to-channel relationship is Many-to-Many. In other words, each show might appear on 
many channels, and each channel might broadcast many shows
* TV show titles do not have spaces
* Channels have 3 letters
* TV show titles can appear multiple times, with different counts
* A TV show and channel combination might appear multiple times

The output should have no commas or punctuation, only spaces between the TV show title and number.

EXECUTION

Opening a text editor to edit the mapper file:

> gedit join2_mapper.py

Checking that the indentations line up correctly:

> more join2_mapper.py

Making the .py files executable:

> chmod +x join2_mapper.py

Doing the same for the reducer file

HDFS CODE

[cloudera@quickstart ~]$ gedit join2_mapper.py
[cloudera@quickstart ~]$ more join2_mapper.py
...
[cloudera@quickstart ~]$ chmod +x join2_mapper.py
[cloudera@quickstart ~]$ gedit join2_reducer.py
[cloudera@quickstart ~]$ more join2_reducer.py
...
[cloudera@quickstart ~]$ chmod +x join2_reducer.py

PYTHON CODE

#                    *** mapper code (join2_mapper.py) ***

#!/usr/bin/env python
import sys					
# --------------------------------------------------------------------------
# This mapper code will input a <TVshow, viewers> input file and a <TVshow, channel> input file, 
# and move date into the value field for output.
# Channels that are not ABC are filtered out.
#
# Note, there is NO error checking of the input, it is assumed to be correct
# meaning no extra spaces, missing inputs or counts, etc..
#
# --------------------------------------------------------------------------
for line in sys.stdin:
    line       = line.strip()              # strip out carriage return
    key_value  = line.split(",")           # split line, into key and value, returns a list
    key_in     = key_value[0]              # key is the first item in the list, contains the TV show
    value_in   = key_value[1]              # value is the 2nd item in the list, contains the number 
	                                       # of viewers or the channel
    # print key_in and value_in                    
    if (value_in.isdigit()) or (value_in == "ABC"): # if the value represents viewers or ABC channel
	    print( '%s\t%s' % (key_in, value_in) ) # print a string tab string

#                    *** reducer code (join2_reducer.py) ***

#!/usr/bin/env python
import sys					
# --------------------------------------------------------------------------
# This reducer code will input a <TVshow, viewers> input file and a <TVshow, channel> input file
# and join TV shows together to provide the total number of viewers for different shows on ABC channel.
# Note that the input will come as a group of lines with the same TV show (the key).
# The code will keep track of the current show and of the previous show. If the show changes
# then it will perform the 'join' on the set of held values by printing out the previous show and the corresponding 
# cumulative value of viewers if the previous show has been transmitted by the ABC channel.  
# At the end it will perform the last join but only if the current key corresponds to a value "ABC".
#
# Note, there is NO error checking of the input, it is assumed to be correct
# meaning no extra spaces, missing inputs or counts, etc..
#
# --------------------------------------------------------------------------
prev_show = " "                    # initialize the variable that contains the show name to blank
show_total = 0                     # initialize the variable that contains the running total for the 
                                   # show to 0
ABC_found = False                  # logical variable indicating that the show is tun on ABC channel
line_cnt = 0                       # count input lines								   
for line in sys.stdin:
    line       = line.strip()      # strip out carriage return
    key_value  = line.split('\t')  # split line, into key and value, returns a list
	line_cnt = line_cnt+1
    curr_show = key_value[0]       # key is the first item in the list, indexed by 0
    value_in = key_value[1]        # value is the 2nd item in the list	
	# Checking if is a new show
	if (curr_show != prev_show) and (line_cnt>1): # if not the first line and the line shows a new show
	    if ABC_found == True: # if a previous line with the previous key had a value equal to "ABC"
	        print( '%s\t%s' % (prev_show, show_total) ) # print the previous show and the total count of viewers
			ABC_found = False # reset the variable
		show_total = 0 # reset the variable
	prev_show = curr_show # set up previous show for the next set of input lines
	# ---------------------------------------------------------------
    # Whether or not the join result was written out, now process the current show.    
    # Determine if its from the file <TVshow, viewers> or from the file <TVshow, channel>
    # and add up the total viewers
    # ---------------------------------------------------------------		
	if value_in == "ABC": # the value is the channel "ABC"
        ABC_found = True
    else: # the value is the number of viewers
	    value_in = int(value_in) # changing the variable into integer before summing it up
        show_total += value_in   # adding the variable that stores the views to the variable that 
	                             # stores the running total for the show						 
# Writing the last join result
if ABC_found == True: # print only if this key has a value equal to "ABC"
    print( '%s\t%s' % (curr_show, show_total) ) # print a string/tab/string      

5. Uploading the resulting output from the reducers, by using numReduceTasks=1.

EXECUTION

Testing:

> cat join2_gen*.txt | ./join2_mapper.py | sort | ./join2_reducer.py

Run the Hadoop streaming command:

> hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
   -input /user/cloudera/input4 \
   -output /user/cloudera/output_join5 \   
   -mapper /home/cloudera/join2_mapper.py \   
   -reducer /home/cloudera/join2_reducer.py
   
HDFS CODE

[cloudera@quickstart ~]$ cat join2_gen*.txt | ./join2_mapper.py | sort | ./join2_reducer.py
Almost_Games	49237
Almost_News	46592
Almost_Show	50202
Baked_Games	51604
Baked_News	47211
Cold_News	47924
Cold_Sports	52005
Dumb_Show	53824
Dumb_Talking	103894
Hot_Games	50228
Hot_Show	54378
Hourly_Cooking	54208
Hourly_Show	48283
Hourly_Talking	108163
Loud_Games	49482
Loud_Show	50820
PostModern_Games	50644
PostModern_News	50021
Surreal_News	50420
Surreal_Sports	46834
[cloudera@quickstart ~]$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
> -input /user/cloudera/input4 \
> -output /user/cloudera/output_join5 \
> -mapper /home/cloudera/join2_mapper.py \
> -reducer /home/cloudera/join2_reducer.py
packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.4.2.jar] /tmp/streamjob3613917503635855419.jar tmpDir=null
18/06/19 13:21:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
18/06/19 13:21:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
18/06/19 13:21:21 INFO mapred.FileInputFormat: Total input paths to process : 6
18/06/19 13:21:21 INFO mapreduce.JobSubmitter: number of splits:6
18/06/19 13:21:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1529389034832_0003
18/06/19 13:21:21 INFO impl.YarnClientImpl: Submitted application application_1529389034832_0003
18/06/19 13:21:21 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1529389034832_0003/
18/06/19 13:21:21 INFO mapreduce.Job: Running job: job_1529389034832_0003
18/06/19 13:21:30 INFO mapreduce.Job: Job job_1529389034832_0003 running in uber mode : false
18/06/19 13:21:30 INFO mapreduce.Job:  map 0% reduce 0%
18/06/19 13:21:54 INFO mapreduce.Job:  map 17% reduce 0%
18/06/19 13:21:58 INFO mapreduce.Job:  map 33% reduce 0%
18/06/19 13:21:59 INFO mapreduce.Job:  map 50% reduce 0%
18/06/19 13:22:00 INFO mapreduce.Job:  map 67% reduce 0%
18/06/19 13:22:01 INFO mapreduce.Job:  map 83% reduce 0%
18/06/19 13:22:02 INFO mapreduce.Job:  map 100% reduce 0%
18/06/19 13:22:06 INFO mapreduce.Job:  map 100% reduce 100%
18/06/19 13:22:06 INFO mapreduce.Job: Job job_1529389034832_0003 completed successfully
18/06/19 13:22:06 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=115133
		FILE: Number of bytes written=1017405
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=113790
		HDFS: Number of bytes written=395
		HDFS: Number of read operations=21
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=6
		Launched reduce tasks=1
		Data-local map tasks=6
		Total time spent by all maps in occupied slots (ms)=148662
		Total time spent by all reduces in occupied slots (ms)=8182
		Total time spent by all map tasks (ms)=148662
		Total time spent by all reduce tasks (ms)=8182
		Total vcore-seconds taken by all map tasks=148662
		Total vcore-seconds taken by all reduce tasks=8182
		Total megabyte-seconds taken by all map tasks=152229888
		Total megabyte-seconds taken by all reduce tasks=8378368
	Map-Reduce Framework
		Map input records=6600
		Map output records=6020
		Map output bytes=103087
		Map output materialized bytes=115163
		Input split bytes=735
		Combine input records=0
		Combine output records=0
		Reduce input groups=54
		Reduce shuffle bytes=115163
		Reduce input records=6020
		Reduce output records=21
		Spilled Records=12040
		Shuffled Maps =6
		Failed Shuffles=0
		Merged Map outputs=6
		GC time elapsed (ms)=1980
		CPU time spent (ms)=4210
		Physical memory (bytes) snapshot=1467211776
		Virtual memory (bytes) snapshot=10518761472
		Total committed heap usage (bytes)=1054433280
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=113055
	File Output Format Counters 
		Bytes Written=395
18/06/19 13:22:06 INFO streaming.StreamJob: Output directory: /user/cloudera/output_join4packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.4.2.jar] /tmp/streamjob8632537748201897449.jar tmpDir=null
18/06/19 16:05:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
18/06/19 16:05:15 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
18/06/19 16:05:16 INFO mapred.FileInputFormat: Total input paths to process : 6
18/06/19 16:05:16 INFO mapreduce.JobSubmitter: number of splits:6
18/06/19 16:05:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1529389034832_0004
18/06/19 16:05:16 INFO impl.YarnClientImpl: Submitted application application_1529389034832_0004
18/06/19 16:05:17 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1529389034832_0004/
18/06/19 16:05:17 INFO mapreduce.Job: Running job: job_1529389034832_0004
18/06/19 16:05:25 INFO mapreduce.Job: Job job_1529389034832_0004 running in uber mode : false
18/06/19 16:05:25 INFO mapreduce.Job:  map 0% reduce 0%
18/06/19 16:05:50 INFO mapreduce.Job:  map 17% reduce 0%
18/06/19 16:05:56 INFO mapreduce.Job:  map 33% reduce 0%
18/06/19 16:05:58 INFO mapreduce.Job:  map 50% reduce 0%
18/06/19 16:05:59 INFO mapreduce.Job:  map 83% reduce 0%
18/06/19 16:06:00 INFO mapreduce.Job:  map 100% reduce 0%
18/06/19 16:06:04 INFO mapreduce.Job:  map 100% reduce 100%
18/06/19 16:06:05 INFO mapreduce.Job: Job job_1529389034832_0004 completed successfully
18/06/19 16:06:05 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=115133
		FILE: Number of bytes written=1017405
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=113790
		HDFS: Number of bytes written=393
		HDFS: Number of read operations=21
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=6
		Launched reduce tasks=1
		Data-local map tasks=6
		Total time spent by all maps in occupied slots (ms)=161528
		Total time spent by all reduces in occupied slots (ms)=9893
		Total time spent by all map tasks (ms)=161528
		Total time spent by all reduce tasks (ms)=9893
		Total vcore-seconds taken by all map tasks=161528
		Total vcore-seconds taken by all reduce tasks=9893
		Total megabyte-seconds taken by all map tasks=165404672
		Total megabyte-seconds taken by all reduce tasks=10130432
	Map-Reduce Framework
		Map input records=6600
		Map output records=6020
		Map output bytes=103087
		Map output materialized bytes=115163
		Input split bytes=735
		Combine input records=0
		Combine output records=0
		Reduce input groups=54
		Reduce shuffle bytes=115163
		Reduce input records=6020
		Reduce output records=21
		Spilled Records=12040
		Shuffled Maps =6
		Failed Shuffles=0
		Merged Map outputs=6
		GC time elapsed (ms)=2311
		CPU time spent (ms)=4490
		Physical memory (bytes) snapshot=1474830336
		Virtual memory (bytes) snapshot=10521292800
		Total committed heap usage (bytes)=1054433280
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=113055
	File Output Format Counters 
		Bytes Written=393
18/06/19 16:06:05 INFO streaming.StreamJob: Output directory: /user/cloudera/output_join5
[cloudera@quickstart ~]$ 
